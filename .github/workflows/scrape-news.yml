name: Scrape Mulhouse News

on:
  schedule:
    - cron: '*/5 * * * *' # Ex√©cution toutes les 5 minutes
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install Node.js dependencies
        run: npm ci

      - name: Generate Prisma Client
        run: npx prisma generate
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 lxml psycopg2-binary python-dotenv curl_cffi googlenewsdecoder

      - name: Run Scraper Script (Discovery)
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          python scripts/scrape_and_seed.py

      - name: Run Full Content & Image Scraper
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          ALSACE_COOKIES: ${{ secrets.ALSACE_COOKIES }}
          B2_ENDPOINT: ${{ secrets.B2_ENDPOINT }}
          B2_APPLICATION_KEY_ID: ${{ secrets.B2_APPLICATION_KEY_ID }}
          B2_APPLICATION_KEY: ${{ secrets.B2_APPLICATION_KEY }}
          B2_BUCKET_NAME: ${{ secrets.B2_BUCKET_NAME }}
          B2_PUBLIC_URL: ${{ secrets.B2_PUBLIC_URL }}
        run: |
          python scripts/scrape_content_full.py

      - name: Log failure to DB
        if: failure()
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          python scripts/log_github_failure.py
