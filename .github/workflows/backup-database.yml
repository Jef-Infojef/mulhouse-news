name: Sauvegarde Base de DonnÃ©es (Supabase)

on:
  schedule:
    # ExÃ©cution 4 fois par jour : 0h, 6h, 12h et 18h UTC
    - cron: '0 0,6,12,18 * * *'
  workflow_dispatch: # Permet de lancer manuellement

jobs:
  backup:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout du code
        uses: actions/checkout@v4

      - name: CrÃ©er le nom du fichier de sauvegarde
        id: backup-file
        run: |
          BACKUP_DATE=$(date +%Y-%m-%d_%H-%M-%S)
          BACKUP_FILE="backup-supabase-${BACKUP_DATE}.sql.gz"
          echo "filename=${BACKUP_FILE}" >> $GITHUB_OUTPUT
          echo "date=${BACKUP_DATE}" >> $GITHUB_OUTPUT

      - name: Installer PostgreSQL 17 Client
        run: |
          # Ajouter le dÃ©pÃ´t PostgreSQL officiel
          sudo apt-get update
          sudo apt-get install -y curl ca-certificates
          sudo install -d /usr/share/postgresql-common/pgdg
          sudo curl -o /usr/share/postgresql-common/pgdg/apt.postgresql.org.asc --fail https://www.postgresql.org/media/keys/ACCC4CF8.asc
          sudo sh -c 'echo "deb [signed-by=/usr/share/postgresql-common/pgdg/apt.postgresql.org.asc] https://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'

          # Installer PostgreSQL 17 client
          sudo apt-get update
          sudo apt-get install -y postgresql-client-17

          # VÃ©rifier la version
          psql --version
          pg_dump --version

      - name: Sauvegarder la base PostgreSQL Supabase
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          set -e
          echo "ðŸ“¦ CrÃ©ation du dump PostgreSQL..."
          
          # VÃ©rification de l'URL
          if [ -z "$DATABASE_URL" ]; then
            echo "âŒ Erreur: DATABASE_URL est vide!"
            exit 1
          fi

          # Ajout de sslmode=require si manquant
          DB_URL="${DATABASE_URL}"
          if [[ "$DB_URL" != *"?sslmode="* ]] && [[ "$DB_URL" != *"&sslmode="* ]]; then
             if [[ "$DB_URL" == *"?"* ]]; then
                DB_URL="${DB_URL}&sslmode=require"
             else
                DB_URL="${DB_URL}?sslmode=require"
             fi
          fi

          # ExÃ©cution de pg_dump d'abord (sans pipe gzip immÃ©diat pour voir les erreurs)
          echo "â³ ExÃ©cution de pg_dump..."
          pg_dump "$DB_URL" --no-owner --no-privileges --clean --if-exists --file=backup.sql

          # VÃ©rification de la taille du fichier SQL
          FILE_SIZE=$(stat -c%s backup.sql 2>/dev/null || echo 0)
          echo "ðŸ“ Taille du fichier SQL brut : $FILE_SIZE octets"
          
          if [ "$FILE_SIZE" -lt 100 ]; then
             echo "âŒ Erreur: Le fichier SQL est trop petit ou vide. Ã‰chec probable de pg_dump."
             cat backup.sql # Affiche le contenu si c'est un message d'erreur texte
             exit 1
          fi

          # Compression
          echo "ðŸ—œï¸ Compression..."
          gzip -c backup.sql > ${{ steps.backup-file.outputs.filename }}
          
          echo "âœ… Sauvegarde compressÃ©e crÃ©Ã©e"
          ls -lh ${{ steps.backup-file.outputs.filename }}
          
          # Nettoyage fichier temporaire
          rm backup.sql

      - name: Configurer les credentials B2
        run: |
          aws configure set aws_access_key_id ${{ secrets.B2_APPLICATION_KEY_ID }}
          aws configure set aws_secret_access_key ${{ secrets.B2_APPLICATION_KEY }}
          aws configure set default.region us-west-002

      - name: Uploader vers B2 Cloud Storage
        env:
          B2_ENDPOINT: s3.eu-central-003.backblazeb2.com
        run: |
          aws s3 cp ${{ steps.backup-file.outputs.filename }} \
            s3://${{ secrets.B2_BUCKET_NAME }}/database-backups/${{ steps.backup-file.outputs.filename }} \
            --endpoint-url=https://$B2_ENDPOINT

      - name: Nettoyer les anciennes sauvegardes (garder 30 jours)
        env:
          B2_ENDPOINT: s3.eu-central-003.backblazeb2.com
        run: |
          aws s3 ls s3://${{ secrets.B2_BUCKET_NAME }}/database-backups/ \
            --endpoint-url=https://$B2_ENDPOINT > backup-list.txt || true

          # Supprimer les sauvegardes de plus de 30 jours
          CUTOFF_DATE=$(date -d '30 days ago' +%Y-%m-%d)

          while IFS= read -r line; do
            # Format: 2024-01-22 14:00:00 123456 backup-supabase-2024-01-22_14-00-00.sql.gz
            FILE_NAME=$(echo "$line" | awk '{print $4}')
            
            # Extraire date du nom de fichier (backup-supabase-YYYY-MM-DD_*.sql.gz)
            FILE_DATE=$(echo "$FILE_NAME" | grep -oP 'backup-supabase-\K[0-9]{4}-[0-9]{2}-[0-9]{2}' || true)

            if [ ! -z "$FILE_DATE" ] && [ "$FILE_DATE" \< "$CUTOFF_DATE" ]; then
              echo "Suppression de l'ancienne sauvegarde : $FILE_NAME"
              aws s3 rm s3://${{ secrets.B2_BUCKET_NAME }}/database-backups/$FILE_NAME \
                --endpoint-url=https://$B2_ENDPOINT || true
            fi
          done < backup-list.txt
