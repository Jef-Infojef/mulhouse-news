# üìò Documentation Technique - Mulhouse News

Ce document d√©taille l'architecture technique, le pipeline d'automatisation et les processus de r√©cup√©ration de donn√©es (scraping) du projet Mulhouse News.

---

## 1. Vue d'ensemble de l'Architecture

Le projet fonctionne sur une architecture hybride **Next.js + Python** :

*   **Frontend & API :** Next.js (App Router) h√©berg√© sur **Vercel**.
*   **Base de Donn√©es :** PostgreSQL g√©r√© par **Neon** (Serverless Postgres).
*   **Stockage Images :** **Backblaze B2** (Object Storage S3-compatible).
*   **Moteur de Scraping :** Scripts Python ex√©cut√©s via **GitHub Actions**.
*   **Orchestration :** D√©clencheur externe via **cron-job.org**.

---

## 2. Syst√®me d'Automatisation (Cron Job)

Pour garantir une fra√Æcheur des donn√©es en temps r√©el (actualisation toutes les 5 minutes), nous contournons le planificateur natif de GitHub (trop impr√©cis) au profit d'un d√©clencheur externe.

### Flux d'Ex√©cution
1.  **Le D√©clencheur (Cron-job.org)** :
    *   Toutes les **5 minutes**, `cron-job.org` envoie une requ√™te HTTP `POST` authentifi√©e.
    *   **Cible :** API GitHub (`api.github.com/repos/.../dispatches`).
    *   **Authentification :** Token personnel GitHub (PAT) avec droits `repo`.

2.  **Le R√©cepteur (GitHub Actions)** :
    *   GitHub re√ßoit l'√©v√©nement `workflow_dispatch`.
    *   Il d√©marre imm√©diatement le workflow d√©fini dans `.github/workflows/scrape-news.yml`.

3.  **L'Ex√©cution (Runner Ubuntu)** :
    *   Installe Python 3.11 et Node.js.
    *   Installe les d√©pendances (`requests`, `beautifulsoup4`, `curl_cffi`, `prisma`, etc.).
    *   Lance s√©quentiellement les scripts de scraping.

---

## 3. Pipeline de Scraping (D√©tail des Scripts)

Le scraping est divis√© en deux phases pour optimiser les performances et r√©duire les appels inutiles.

### Phase 1 : D√©couverte (`scripts/scrape_and_seed.py`)
*   **R√¥le :** Trouver les nouvelles URLs d'articles.
*   **Sources :** Google News (RSS), Flux RSS locaux, Pages d'accueil (DNA, L'Alsace, M+, etc.).
*   **M√©thode :**
    *   R√©cup√®re les listes d'articles.
    *   V√©rifie l'existence de chaque URL en base de donn√©es (pour √©viter les doublons).
    *   Ins√®re les **nouveaux articles** avec les infos de base (Titre, URL, Date, Source).
    *   *Si aucun nouvel article n'est trouv√©, le processus s'arr√™te souvent ici.*

### Phase 2 : Enrichissement (`scripts/scrape_content_full.py`)
*   **R√¥le :** R√©cup√©rer le contenu complet et les images pour les articles incomplets.
*   **Cible :** Articles en base o√π `content` est `NULL`.
*   **Technologie :**
    *   Utilise `curl_cffi` pour simuler un vrai navigateur (Chrome) et contourner les protections anti-bot (Cloudflare, Datadome).
    *   Utilise `GoogleNewsDecoder` pour r√©soudre les liens Google News obfusqu√©s.
*   **Gestion des Images :**
    *   T√©l√©charge l'image source.
    *   L'upload vers **Backblaze B2**.
    *   G√©n√®re une URL publique (CDN) et met √† jour l'article en base.

---

## 4. Gestion des Erreurs et Logs

*   **Logs GitHub :** Chaque ex√©cution est visible dans l'onglet "Actions" du d√©p√¥t.
*   **Logs Base de Donn√©es :**
    *   En cas d'√©chec critique du script Python, un script de secours (`scripts/log_github_failure.py`) est appel√©.
    *   Il enregistre l'erreur dans la table `ScrapingLog` de la base de donn√©es.
*   **Interface Admin :** Les logs sont consultables via l'interface web du site (`/admin/logs`).

---

## 5. Configuration Technique Requise

### Variables d'Environnement (Secrets GitHub)
Le bon fonctionnement d√©pend des cl√©s suivantes dans GitHub Secrets :

*   `DATABASE_URL` : URL de connexion PostgreSQL (Neon).
*   `ALSACE_COOKIES` : Cookies de session (si n√©cessaire pour acc√®s abonn√©).
*   `B2_APPLICATION_KEY_ID` & `B2_APPLICATION_KEY` : Identifiants Backblaze.
*   `B2_BUCKET_NAME` : Nom du bucket de stockage.

### D√©pendances Cl√©s (Python)
*   `curl_cffi` : Client HTTP impersonnel (imite les navigateurs).
*   `beautifulsoup4` : Parsing HTML.
*   `psycopg2-binary` : Connexion PostgreSQL.
*   `googlenewsdecoder` : D√©codage des URLs Google News.

---

## 6. Maintenance

*   **Changement de fr√©quence :** Modifier le job sur `cron-job.org`.
*   **Arr√™t d'urgence :** D√©sactiver le workflow dans l'onglet Actions de GitHub ou mettre en pause le job sur `cron-job.org`.
*   **Mise √† jour des s√©lecteurs CSS :** Si un site source change son design, il faut mettre √† jour les classes CSS dans `scripts/scrape_content_full.py`.
